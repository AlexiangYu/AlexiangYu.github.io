<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="I love to share my thoughts and experiences about technology, programming and life.">
    <meta name="keyword" content="">
    <meta name="theme-color" content="#600090">
    <meta name="msapplication-navbutton-color" content="#600090">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="#600090">
    <link rel="shortcut icon" href="https://cdn4.iconfinder.com/data/icons/ionicons/512/icon-person-128.png">
    <link rel="alternate" type="application/atom+xml" title="Alexiang" href="/atom.xml">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css">
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.css">
    <title>
        
        使用LangChain构建LLMs应用｜undefined
        
    </title>

    <link rel="canonical" href="http://example.com/2024/11/09/使用LangChain构建LLMs应用/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/blog-style.css">


    <!-- Pygments Github CSS -->
    
<link rel="stylesheet" href="/css/syntax.css">

<meta name="generator" content="Hexo 7.2.0"></head>

<style>

    header.intro-header {
        background-image: url('')
    }
</style>
<!-- hack iOS CSS :active style -->
<body ontouchstart="" class="animated fadeIn">
<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top " id="nav-top" data-ispost = "true" data-istags="false
" data-ishome = "false" >
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand animated pulse" href="/">
                <span class="brand-logo">
                    Alexiang
                </span>
                's Blog
            </a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <!-- /.navbar-collapse -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
					
                    
					
					
                </ul>
            </div>
        </div>
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
//    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>

<!-- Main Content -->

<!--only post-->


<img class="wechat-title-img"
     src="">


<style>
    
    header.intro-header {
        background-image: url('')
    }

    
</style>

<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 text-center">
                <div class="post-heading">
                    <h1>使用LangChain构建LLMs应用</h1>
                    
                    <span class="meta">
                         作者 Alexiang
                        <span>
                          日期 2024-11-09
                         </span>
                    </span>
                    <div class="tags text-center">
                        
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="post-title-haojen">
        <span>
            使用LangChain构建LLMs应用
        </span>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Container -->
            <div class="col-lg-8 col-lg-offset-1 col-sm-9 post-container">
                <h2 id="LangChain功能总结"><a href="#LangChain功能总结" class="headerlink" title="LangChain功能总结"></a>LangChain功能总结</h2><ol>
<li>Model I&#x2F;O </li>
<li>RAG</li>
<li>Agent</li>
<li>Chain (LCEL)</li>
<li>Memory</li>
<li>Callbacks</li>
</ol>
<p><strong>使用 LCEL</strong></p>
<p>构造链 LangChain Expression Language 声明式表达方式，轻松组合多个步骤</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chain = prompt | model | output_parser</span><br><span class="line">result = chain.invoke(&#123;<span class="string">&quot;flower&quot;</span>: <span class="string">&quot;玫瑰&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>

<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本质上就是对各种大模型提供的API的套壳，是为了方便我们使用这些API，搭建起来的一些框架、模块和接口。</p>
<blockquote>
<p>LangChain是一个基于大语言模型（LLMs）用于构建端到端语言模型应用的框架，它可以让开发者使用语言模型来实现各种复杂的任务，例如文本到图像的生成、文档问答、聊天机器人等。LangChain提供了一系列工具、套件和接口，可以简化创建由LLMs和聊天模型提供支持的应用程序的过程。</p>
</blockquote>
<p><img src="/ffb488f5f695467bb994245df8d947c1~tplv-k3u1fbpfcp-jj-mark_3024_0_0_0_q75.webp" alt="alt text"></p>
<h2 id="Model-I-O-过程"><a href="#Model-I-O-过程" class="headerlink" title="Model I&#x2F;O 过程"></a>Model I&#x2F;O 过程</h2><p>输入提示（Format）-&gt; 调用模型（Predict）-&gt; 输出解析（Parse）</p>
<ol>
<li>提示模板：使用模型的第一个环节是把提示信息输入到模型中，你可以创建LangChain模板，根据实际需求动态选择不同的输入，针对特定的任务和应用调整输入。</li>
<li>语言模型：LangChain允许你通过通用接口来调用语言模型。这意味着无论你要使用的是哪种语言模型，都可以通过同一种方式进行调用，这样就提高了灵活性和便利性。</li>
<li>输出解析：LangChain还提供了从模型输出中提取信息的功能。通过输出解析器，你可以精确地从模型的输出中获取需要的信息，而不需要处理冗余或不相关的数据，更重要的是还可以把大模型给回的非结构化文本，转换成程序可以处理的结构化数据。</li>
</ol>
<p>LangChain中支持的三大类语言模型:</p>
<ol>
<li>大语言模型（LLM） ，即Text Model，这些模型将文本字符串作为输入，并返回文本字符串作为输出。</li>
<li>聊天模型（Chat Model），主要代表Open AI的ChatGPT系列模型。这些模型通常由语言模型支持，但它们的 API 更加结构化。具体来说，这些模型将聊天消息列表作为输入，并返回聊天消息。</li>
<li>文本嵌入模型（Embedding Model），这些模型将文本作为输入并返回浮点数列表，也就是Embedding。而文本嵌入模型如OpenAI的text-embedding-ada-002。文本嵌入模型负责把文档存入向量数据库。</li>
</ol>
<p>LangChain Agent（代理）实例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---- Part 0 导入所需要的类</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BlipProcessor, BlipForConditionalGeneration</span><br><span class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> BaseTool</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> initialize_agent, AgentType</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---- Part I 初始化图像字幕生成模型</span></span><br><span class="line"><span class="comment"># 指定要使用的工具模型（HuggingFace中的image-caption模型）</span></span><br><span class="line">hf_model = <span class="string">&quot;Salesforce/blip-image-captioning-large&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;正在初始化图像字幕生成模型...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化处理器和工具模型</span></span><br><span class="line"><span class="comment"># 预处理器将准备图像供模型使用</span></span><br><span class="line">processor = BlipProcessor.from_pretrained(hf_model)</span><br><span class="line"><span class="comment"># 然后我们初始化工具模型本身</span></span><br><span class="line">model = BlipForConditionalGeneration.from_pretrained(hf_model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;初始化图像字幕生成模型成功&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---- Part II 定义图像字幕生成工具类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImageCapTool</span>(<span class="title class_ inherited__">BaseTool</span>):</span><br><span class="line">    name = <span class="string">&quot;Image captioner&quot;</span></span><br><span class="line">    description = <span class="string">&quot;使用该工具可以生成图片的文字描述，需要传入图片的URL.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_run</span>(<span class="params">self, url: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="comment"># 下载图像并将其转换为PIL对象</span></span><br><span class="line">        image = Image.<span class="built_in">open</span>(requests.get(url, stream=<span class="literal">True</span>).raw).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        <span class="comment"># 预处理图像</span></span><br><span class="line">        inputs = processor(image, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">        <span class="comment"># 生成字幕</span></span><br><span class="line">        out = model.generate(**inputs, max_new_tokens=<span class="number">200</span>)</span><br><span class="line">        <span class="comment"># 获取字幕</span></span><br><span class="line">        caption = processor.decode(out[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> caption</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_arun</span>(<span class="params">self, query: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;This tool does not support async&quot;</span>)</span><br><span class="line"></span><br><span class="line">agent = initialize_agent(</span><br><span class="line">    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,</span><br><span class="line">    tools=[ImageCapTool()],</span><br><span class="line">    llm=ChatOpenAI( model=os.environ.get(<span class="string">&quot;LLM_MODEL_4K_FUNCTION_CALL&quot;</span>), temperature=<span class="number">0</span>),</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">    handle_parsing_errors=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">img_url = <span class="string">&quot;https://youimg1.c-ctrip.com/target/100v1d000001ej8qh6217.jpg&quot;</span></span><br><span class="line">agent.run(<span class="built_in">input</span>=<span class="string">f&quot;<span class="subst">&#123;img_url&#125;</span>\n请创作合适的中文推广文案&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/get_started">API 文档</a></p>
<h3 id="提示工程"><a href="#提示工程" class="headerlink" title="提示工程"></a>提示工程</h3><p>好的提示（其实也就是好的问题或指示啦），肯定能够让你在调用语言模型的时候事半功倍。</p>
<blockquote>
<p>GPT 最佳实践: </p>
<ol>
<li>写清晰的指示</li>
<li>给模型提供参考（也就是示例）</li>
<li>将复杂任务拆分成子任务</li>
<li>给GPT时间思考</li>
<li>使用外部工具</li>
<li>反复迭代问题</li>
</ol>
</blockquote>
<p>LangChain 提供了多个类和函数，也为各种应用场景设计了很多内置模板，使构建和使用提示变得容易。</p>
<blockquote>
<p>LangChain的优势所在：我们只需要定义一次模板，就可以用它来生成各种不同的提示。</p>
<p><code>PromptTemplate</code> 的 <code>from_template</code> 方法就是将一个原始的模板字符串转化为一个更丰富、更方便操作的 <code>PromptTemplate</code> 对象，这个对象就是LangChain中的提示模板。<br>LangChain中，<code>SemanticSimilarityExampleSelector</code> 对象可以根据语义相似性选择最相关的示例， <code>example_selector</code> 会根据语义的相似度（余弦相似度）找到最相似的示例</p>
</blockquote>
<p>提示框架：指令 + 上下文 + 提示输入 + 输出指示器</p>
<p>GPT-3模型，作为一个大型的自我监督学习模型，通过提升模型规模，实现了出色的<strong>Few-Shot</strong>学习性能。</p>
<p><img src="/357e9ca0ce2b4699a24e3fe512c047ca.webp" alt="alt text"></p>
<h4 id="使用-Chain-of-Thought"><a href="#使用-Chain-of-Thought" class="headerlink" title="使用 Chain of Thought"></a>使用 Chain of Thought</h4><p>《自我一致性提升了语言模型中的思维链推理能力》：如果生成一系列的中间推理步骤，就能够显著提高大型语言模型进行复杂推理的能力。</p>
<blockquote>
<p>Few-Shot CoT 简单的在提示中提供了一些链式思考示例（Chain-of-Thought Prompting），足够大的语言模型的推理能力就能够被增强。简单说，就是给出一两个示例，然后在示例中写清楚推导的过程。</p>
</blockquote>
<p>提供思考示例：问题理解 -&gt; 信息搜索 -&gt; 决策制定 -&gt; 生成结果列表</p>
<blockquote>
<p>Few-Shot CoT，指的就是在带有示例的提示过程中，加入思考的步骤，从而引导模型给出更好的结果。而 Zero-Shot CoT，就是直接告诉模型要一步一步地思考，慢慢地推理。</p>
</blockquote>
<blockquote>
<p>CoT 应当被用于 20B 以上参数规模的模型之中，并且模型的训练数据应当于任务问题相关且彼此相互有较强的联结。 首从工程的角度而言，CoT 的适用场景抽象一下可以被归纳为三点，分别是使用大模型（1），任务需要复杂推理（2），参数量的增加无法使得模型性能显著提升（3）。此外，现有的论文实验也表明，CoT 更加适合复杂的推理任务，比如计算或编程，不太适用于简单的单项选择、序列标记等任务之中，并且 CoT 并不适用于那些参数量较小的模型（20B以下），在小模型中使用 CoT 非常有可能会造成机器幻觉等等问题。 而从理论角度，一篇来自斯坦福的论文《Why think step-by-step? reasoning emerges from the locality of experience》揭示了<strong>当大模型的训练数据表现出了变量的局部簇结构（Local Clusters of Variables）</strong>时，CoT 将会展现极好的效果。而变量的局部簇主要指训练数据中变量之间有着强的相互作用，相互影响的关系。</p>
</blockquote>
<p>需要深度推理的复杂任务：dfs思维树（Tree of Thoughts，ToT）</p>
<blockquote>
<p>请你模拟三位出色、逻辑性强的专家合作回答一个问题。每个人都详细地解释他们的思考过程，考虑到其他人之前的解释，并公开承认错误。在每一步，只要可能，每位专家都会在其他人的思考基础上进行完善和建设，并承认他们的贡献。他们继续，直到对问题有一个明确的答案。为了清晰起见，您的整个回应应该是一个Markdown表格。   问题是…</p>
</blockquote>
<h3 id="调用-OpenAI-API"><a href="#调用-OpenAI-API" class="headerlink" title="调用 OpenAI API"></a>调用 OpenAI API</h3><ul>
<li>Text模型：<code>prompt</code> 与 <code>max_tokens</code> 参数控制生成的文本长度。简洁。</li>
<li>Chat模型：消息角色：<ul>
<li><code>system</code>：系统消息主要用于设定对话的背景或上下文。这可以帮助模型理解它在对话中的角色和任务。系统消息通常在对话开始时给出。</li>
<li><code>user</code>：用户消息是从用户或人类角色发出的。它们通常包含了用户想要模型回答或完成的请求。</li>
<li><code>assistant</code>：助手消息是模型的回复。发送多轮对话中新的对话请求时，可以通过助手消息提供先前对话的上下文。在对话的最后一条消息应始终为用户消息。</li>
</ul>
</li>
</ul>
<p>响应对象：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> <span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;chatcmpl-2nZI6v1cW9E3Jg4w2Xtoql0M3XHfH&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;object&#x27;</span>: <span class="string">&#x27;chat.completion&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;created&#x27;</span>: <span class="number">1677649420</span>,</span><br><span class="line"> <span class="string">&#x27;model&#x27;</span>: <span class="string">&#x27;gpt-4&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;usage&#x27;</span>: &#123;<span class="string">&#x27;prompt_tokens&#x27;</span>: <span class="number">56</span>, <span class="string">&#x27;completion_tokens&#x27;</span>: <span class="number">31</span>, <span class="string">&#x27;total_tokens&#x27;</span>: <span class="number">87</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;choices&#x27;</span>: [</span><br><span class="line">   &#123;</span><br><span class="line">    <span class="string">&#x27;message&#x27;</span>: &#123;</span><br><span class="line">      <span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;assistant&#x27;</span>,</span><br><span class="line">      <span class="string">&#x27;content&#x27;</span>: <span class="string">&#x27;你的花店可以叫做&quot;花香四溢&quot;。&#x27;</span></span><br><span class="line">     &#125;,</span><br><span class="line">    <span class="string">&#x27;finish_reason&#x27;</span>: <span class="string">&#x27;stop&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;index&#x27;</span>: <span class="number">0</span></span><br><span class="line">   &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>text</code>：生成的文本。</li>
<li><code>finish_reason</code>：模型停止生成的原因，可能的值包括 stop（遇到了停止标记）、length（达到了最大长度）或 temperature（根据设定的温度参数决定停止）。</li>
<li><code>created</code>：生成响应的时间戳。</li>
</ul>
<p>使用OpenAI API</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的OpenAI API Key&#x27;</span></span><br><span class="line"><span class="comment"># os.environ[&quot;OPENAI_BASE_URL&quot;] = &#x27;OpenAI 的 API URL&#x27;</span></span><br><span class="line"></span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># text = client.invoke(&quot;请给我写一句情人节红玫瑰的中文宣传语&quot;)</span></span><br><span class="line">response = client.chat.completions.create(</span><br><span class="line">    model=os.environ.get(<span class="string">&quot;LLM_MODELEND&quot;</span>),</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a creative AI.&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;请给我的花店起个名&quot;</span>&#125;,</span><br><span class="line">    ],</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">600</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure>

<p>通过 LangChain 调用</p>
<p>我们只需要定义一次模板，更简洁。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.schema <span class="keyword">import</span> HumanMessage, SystemMessage</span><br><span class="line"></span><br><span class="line"><span class="comment"># os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的OpenAI API Key&#x27;</span></span><br><span class="line"><span class="comment"># os.environ[&quot;OPENAI_BASE_URL&quot;] = &#x27;OpenAI 的 API URL&#x27;</span></span><br><span class="line"></span><br><span class="line">chat = ChatOpenAI(model=os.environ.get(<span class="string">&quot;LLM_MODELEND&quot;</span>), temperature=<span class="number">0.8</span>, max_tokens=<span class="number">600</span>)</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=<span class="string">&quot;你是一个很棒的智能助手&quot;</span>),</span><br><span class="line">    HumanMessage(content=<span class="string">&quot;请给我的猫起个名&quot;</span>),</span><br><span class="line">]</span><br><span class="line">response = chat(messages)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>


<p><strong>预训练+微调的大模型应用模式</strong>：高效、扩展性</p>
<blockquote>
<p>微调（Fine-tuning）：对模型的头部或者部分参数根据自己的需要进行适应性的调整，这通常涉及在相对较小的有标注数据集上进行有监督学习，让模型适应特定任务的需求。<br>微调过程相比于从头训练一个模型要快得多，且需要的数据量也要少得多。</p>
</blockquote>
<ul>
<li>预训练：在大规模无标注文本数据上进行模型的训练，学习基本的特征和模式。</li>
<li>微调：快速地根据特定任务进行优化，使用标注好的数据集，以便模型能针对特定任务进行优化和调整。</li>
</ul>
<p>从HuggingFace下载并导入模型使用</p>
<h3 id="输出解析"><a href="#输出解析" class="headerlink" title="输出解析"></a>输出解析</h3><p>得到清晰的数据结构，方便将解析好的数据存入CSV文档。</p>
<p>输出解析器类要实现两个核心方法：</p>
<ul>
<li><code>get_format_instructions</code>：这个方法需要返回一个字符串，用于指导如何格式化语言模型的输出，告诉它应该如何组织并构建它的回答。</li>
<li><code>parse</code>：这个方法接收一个字符串（也就是语言模型的输出）并将其解析为特定的数据结构或格式。这一步通常用于确保模型的输出符合我们的预期，并且能够以需要的形式进行后续处理。</li>
<li><code>parse_with_prompt</code>：这个方法接收一个字符串（也就是语言模型的输出）和一个提示（用于生成这个输出的提示），并将其解析为特定的数据结构。这样，你可以根据原始提示来修正或重新解析模型的输出，确保输出的信息更加准确和贴合要求。</li>
</ul>
<blockquote>
<p>用 <code>output_parser.parse(output)</code> 把模型输出的文案解析成之前定义好的数据格式，也就是一个Python字典，这个字典中包含了 <code>description</code> 和 <code>reason</code> 这两个字段的值。</p>
</blockquote>
<p><strong>使用 Pydantic（JSON）解析器</strong>: 数据验证、数据转换</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;Your OpenAI API Key&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(</span><br><span class="line">    model=os.environ.get(<span class="string">&quot;LLM_MODELEND&quot;</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------Part 2</span></span><br><span class="line"><span class="comment"># 创建一个空的DataFrame用于存储结果</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(columns=[<span class="string">&quot;flower_type&quot;</span>, <span class="string">&quot;price&quot;</span>, <span class="string">&quot;description&quot;</span>, <span class="string">&quot;reason&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line">flowers = [<span class="string">&quot;玫瑰&quot;</span>, <span class="string">&quot;百合&quot;</span>, <span class="string">&quot;康乃馨&quot;</span>]</span><br><span class="line">prices = [<span class="string">&quot;50&quot;</span>, <span class="string">&quot;30&quot;</span>, <span class="string">&quot;20&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义我们想要接收的数据格式</span></span><br><span class="line"><span class="keyword">from</span> pydantic.v1 <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FlowerDescription</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    flower_type: <span class="built_in">str</span> = Field(description=<span class="string">&quot;鲜花的种类&quot;</span>)</span><br><span class="line">    price: <span class="built_in">int</span> = Field(description=<span class="string">&quot;鲜花的价格&quot;</span>)</span><br><span class="line">    description: <span class="built_in">str</span> = Field(description=<span class="string">&quot;鲜花的描述文案&quot;</span>)</span><br><span class="line">    reason: <span class="built_in">str</span> = Field(description=<span class="string">&quot;为什么要这样写这个文案&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------Part 3</span></span><br><span class="line"><span class="comment"># 创建输出解析器</span></span><br><span class="line"><span class="keyword">from</span> langchain.output_parsers <span class="keyword">import</span> PydanticOutputParser</span><br><span class="line"></span><br><span class="line">output_parser = PydanticOutputParser(pydantic_object=FlowerDescription)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取输出格式指示</span></span><br><span class="line">format_instructions = output_parser.get_format_instructions()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出格式：&quot;</span>, format_instructions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------Part 4</span></span><br><span class="line"><span class="comment"># 创建提示模板</span></span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">prompt_template = <span class="string">&quot;&quot;&quot;您是一位专业的鲜花店文案撰写员。</span></span><br><span class="line"><span class="string">对于售价为 &#123;price&#125; 元的 &#123;flower&#125; ，您能提供一个吸引人的简短中文描述吗？</span></span><br><span class="line"><span class="string">&#123;format_instructions&#125;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据模板创建提示，同时在提示中加入输出解析器的说明</span></span><br><span class="line">prompt = PromptTemplate.from_template(</span><br><span class="line">    prompt_template, partial_variables=&#123;<span class="string">&quot;format_instructions&quot;</span>: format_instructions&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印提示</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;提示：&quot;</span>, prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------Part 5</span></span><br><span class="line"><span class="keyword">for</span> flower, price <span class="keyword">in</span> <span class="built_in">zip</span>(flowers, prices):</span><br><span class="line">    <span class="comment"># 根据提示准备模型的输入</span></span><br><span class="line">    <span class="built_in">input</span> = prompt.<span class="built_in">format</span>(flower=flower, price=price)</span><br><span class="line">    <span class="comment"># 打印提示</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;提示：&quot;</span>, <span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取模型的输出</span></span><br><span class="line">    output = model.predict(<span class="built_in">input</span>)</span><br><span class="line">    <span class="comment"># 解析模型的输出</span></span><br><span class="line">    parsed_output = output_parser.parse(output)</span><br><span class="line">    parsed_output_dict = parsed_output.<span class="built_in">dict</span>()  <span class="comment"># 将Pydantic格式转换为字典</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将解析后的输出添加到DataFrame中</span></span><br><span class="line">    df.loc[<span class="built_in">len</span>(df)] = parsed_output.<span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出的数据：&quot;</span>, df.to_dict(orient=<span class="string">&quot;records&quot;</span>))</span><br></pre></td></tr></table></figure>

<ul>
<li>自动修复解析器 <code>OutputFixingParser</code>：调用了原有的PydanticOutputParser，如果失败，它会将格式错误的输出以及格式化的指令传递给大模型，并要求LLM进行相关的修复。</li>
<li>重试解析器 <code>RetryWithErrorOutputParser</code>：利用大模型的推理能力根据原始提示找回相关信息。</li>
</ul>
<p>文档：LangChain中的各种 <a href="https://link.juejin.cn/?target=https://python.langchain.com/docs/modules/model_io/output_parsers/">Output Parsers</a></p>
<h2 id="Chain"><a href="#Chain" class="headerlink" title="Chain"></a>Chain</h2><blockquote>
<p>LangChain通过设计好的接口，实现一个具体的链的功能。例如，LLM链（LLMChain）能够接受用户输入，使用 PromptTemplate 对其进行格式化，然后将格式化的响应传递给 LLM。这就相当于把整个 Model I&#x2F;O 的流程封装到链里面；多个链组可以合在一起。</p>
</blockquote>
<p>多种类型的预置链：</p>
<ol>
<li><strong>LLMChain</strong>：这是最简单的链，主要用于处理基本的对话任务、文本生成等。它将输入文本传递给模型并输出结果。</li>
<li><strong>SequentialChain</strong>：将多个链顺序调用。每个链的输出可以作为下一个链的输入。</li>
<li><strong>TransformChain</strong>：对输入进行格式转换、分化</li>
<li><strong>RouterChain</strong>：根据输入的条件，选择不同的链。</li>
</ol>
<p><code>LLMChain</code> 围绕着语言模型推理功能又添加了一些功能，整合了 <code>PromptTemplate</code>、语言模型（LLM或聊天模型）和 <code>Output Parser</code>，相当于把Model I&#x2F;O放在一个链中整体操作。它使用提示模板格式化输入，将格式化的字符串传递给 LLM，并返回 LLM 输出。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的库</span></span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> PromptTemplate, OpenAI, LLMChain</span><br><span class="line"><span class="comment"># 原始字符串模板</span></span><br><span class="line">template = <span class="string">&quot;&#123;flower&#125;的花语是?&quot;</span></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 创建LLMChain</span></span><br><span class="line">llm_chain = LLMChain(</span><br><span class="line">    llm=llm,</span><br><span class="line">    prompt=PromptTemplate.from_template(template))</span><br><span class="line"><span class="comment"># 调用LLMChain，返回结果</span></span><br><span class="line">result = llm_chain(<span class="string">&quot;玫瑰&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<p>路由链：构造提示信息，来引导大模型查看用户输入的问题并确定问题的类型的。</p>
<p><code>MultiPromptChain</code> 类把前几个链整合在一起，实现路由功能。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建多提示链</span></span><br><span class="line"><span class="keyword">from</span> langchain.chains.router <span class="keyword">import</span> MultiPromptChain</span><br><span class="line">chain = MultiPromptChain(</span><br><span class="line">    router_chain=router_chain,</span><br><span class="line">    destination_chains=chain_map,</span><br><span class="line">    default_chain=default_chain,</span><br><span class="line">    verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="ConversationChain"><a href="#ConversationChain" class="headerlink" title="ConversationChain"></a>ConversationChain</h3><p>包含AI前缀与人类前缀的对话摘要格式，这个对话格式和记忆机制结合得非常紧密。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 打印对话的模板</span><br><span class="line">The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. # 减少幻觉</span><br><span class="line"></span><br><span class="line">Current conversation:</span><br><span class="line">&#123;history&#125;</span><br><span class="line">Human: &#123;input&#125;</span><br><span class="line">AI:</span><br></pre></td></tr></table></figure>

<ul>
<li><code>&#123;history&#125;</code> 是存储会话记忆的地方，也就是人类和人工智能之间对话历史的信息。</li>
<li><code>&#123;input&#125;</code> 是新输入的地方，你可以把它看成是和ChatGPT对话时，文本框中的输入。</li>
</ul>
<p>加入记忆功能：<code>conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())</code> 。聊天历史信息，都被传入了ConversationChain的提示模板中的 {history} 参数，构建出了包含聊天记录的新的提示输入。</p>
<ul>
<li><code>ConversationBufferWindowMemory</code>：缓冲窗口记忆，它的思路就是只保存最新最近的几次人类和AI的互动。因此，它在之前的“缓冲记忆”基础上增加了一个窗口值 <code>k</code>。这意味着我们只保留一定数量的过去互动，然后“忘记”之前的互动。</li>
<li><code>ConversationSummaryMemory</code>：使用LLM将对话历史进行汇总，然后再传递给 {history} 参数。这种方法旨在通过对之前的对话进行汇总来避免长对话过度使用Token。</li>
<li><code>ConversationSummaryBufferMemory</code>：混合记忆模型，当最新的对话文字长度在300字之内的时候，LangChain会记忆原始对话内容；当对话文字超出了这个参数的长度，那么模型就会把所有超过预设长度的内容进行总结。</li>
</ul>
<p><img src="/c56yyd7eb61637687de448512yy426ea.webp" alt="alt text"></p>
<h2 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h2><p>与外部环境交互，将额外信息合并到推理中。</p>
<ul>
<li>大模型：提供逻辑的引擎，负责生成预测和处理输入。</li>
<li>与之交互的外部工具：可能包括数据清洗工具、搜索引擎、应用程序等。</li>
<li>控制交互的<strong>代理</strong>：调用适当的外部工具，并管理整个交互过程的流程。</li>
</ul>
<h4 id="ReAct框架"><a href="#ReAct框架" class="headerlink" title="ReAct框架"></a>ReAct框架</h4><p>Reasoning-Acting框架：指导大语言模型推理和行动的一种思维框架。</p>
<p>行动 -&gt; 观察 -&gt; 思考 -&gt; 行动</p>
<p>《ReAct：在语言模型中协同推理和行动》：</p>
<blockquote>
<p>大语言模型可以通过生成推理痕迹和任务特定行动来实现更大的协同作用。<br>Reasoning包括了对当前环境和状态的观察，并生成推理轨迹。这使模型能够诱导、跟踪和更新操作计划，甚至处理异常情况。Acting在于指导大模型采取下一步的行动，比如与外部源（如知识库或环境）进行交互并且收集信息，或者给出最终答案。</p>
</blockquote>
<p><img src="/56fbe79e086052895f301383c27f4a0c.webp" alt="ReAct 思维链条"></p>
<blockquote>
<p>在链中，一系列操作被硬编码（在代码中）。在代理中，LLM被用作推理引擎来确定要采取哪些操作以及按什么顺序执行这些操作。</p>
</blockquote>
<p>思考框架：</p>
<blockquote>
<p>Use the following format:\n\n （指导模型使用下面的格式）<br>Question: the input question you must answer\n （问题）<br>Thought: you should always think about what to do\n （思考）<br>Action: the action to take, should be one of [Search, Calculator]\n （行动）<br>Action Input: the input to the action\n （行动的输入）<br>Observation: the result of the action\n… （观察：行动的返回结果）<br>(this Thought&#x2F;Action&#x2F;Action Input&#x2F;Observation can repeat N times)\n （上面这个过程可以重复多次）<br>Thought: I now know the final answer\n （思考：现在我知道最终答案了）<br>Final Answer: the final answer to the original input question\n\n （最终答案）</p>
</blockquote>
<p>根据现有知识解决不了，下一步行动是需要选择工具箱中的搜索工具：</p>
<blockquote>
<p>text: I need to find the current market price of roses and then calculate the new price with a 15% markup.\n （Text：问题文本）<br>Action: Search\n （行动：搜索）<br>Action Input: “Average price of roses” （行动的输入：搜索玫瑰平均价格）</p>
</blockquote>
<p>计算：</p>
<blockquote>
<p>AgentAction(tool&#x3D;’Calculator’, tool_input&#x3D;’80.16 * 1.15’, log&#x3D;’ I need to calculate the new price with a 15% markup.\nAction: Calculator\nAction Input: 80.16 * 1.15’)</p>
</blockquote>
<p>最后，AgentExcutor的plan方法返回一个 <code>AgentFinish</code> 实例，这表示代理经过对输出的检查，其内部逻辑判断出任务已经完成，思考和行动的循环要结束了。</p>
<blockquote>
<p>Thought: I now know the final answer. </p>
</blockquote>
<ul>
<li><p><strong>结构化工具对话代理</strong>（Structured Tool Chat）是一种通过预先设定的结构与用户进行交互的工具，可以有效引导对话的主题和内容。</p>
<ul>
<li>文件管理工具集</li>
<li>Web 浏览器工具集<ul>
<li><code>PlayWright</code> 工具包</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>自主询问搜索代理</strong>（Self-Ask with Search）利用一种叫做 “Follow-up Question（追问）”加“Intermediate Answer（中间答案）”的技巧，来辅助大模型寻找事实性问题的过渡性答案，从而引出最终答案。</p>
<ul>
<li>逐步逼近，以自己提问并搜索：<code>使用玫瑰作为国花的国家的首都是哪里? = 使用玫瑰作为国花的国家？ + 都铎王朝的首都在哪里？</code></li>
</ul>
</li>
<li><p><strong>计划与执行代理</strong>（Plan and Execute）是指能够根据用户的需求制定计划，并执行相应操作的工具，帮助用户实现特定的目标或任务。</p>
<ul>
<li>首先，制定一个计划，并将整个任务划分为更小的子任务；然后按照该计划执行子任务。<ul>
<li>计划由一个LLM代理（负责推理）完成。</li>
<li>执行由另一个LLM代理（负责调用工具）完成。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="使用-arXiv-工具"><a href="#使用-arXiv-工具" class="headerlink" title="使用 arXiv 工具"></a>使用 arXiv 工具</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 导入库</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> load_tools, initialize_agent, AgentType</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型和工具</span></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0.0</span>, model=os.environ.get(<span class="string">&quot;LLM_MODELEND&quot;</span>))</span><br><span class="line">tools = load_tools([<span class="string">&quot;arxiv&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化链</span></span><br><span class="line">agent_chain = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行链</span></span><br><span class="line">agent_chain.run(<span class="string">&quot;介绍一下2005.14165这篇论文的创新点?&quot;</span>)</span><br><span class="line"><span class="comment"># Thought: 论文的创新点是在少样本学习设置下测试了 GPT-3 的性能，发现扩大语言模型的规模可以极大地提高任务无关的少样本性能，有时甚至可以与之前最先进的微调方法竞争。</span></span><br></pre></td></tr></table></figure>


<p><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/tools/">LangChain中集成的所有工具</a></p>
<h2 id="RAG-应用架构"><a href="#RAG-应用架构" class="headerlink" title="RAG 应用架构"></a>RAG 应用架构</h2><blockquote>
<p><code>Retrieval-Augmented Generation</code>，即检索增强生成，它结合了检索和生成的能力，为文本序列生成任务引入外部知识。RAG将传统的语言生成模型与大规模的外部知识库相结合，使模型在生成响应或文本时可以动态地从这些知识库中检索相关信息。</p>
</blockquote>
<p>数据源 - 大模型应用 - 用例</p>
<p>数据处理管道（Pipeline）</p>
<ol>
<li>Loading：文档加载器把 Documents 加载为以LangChain能够读取的形式。</li>
<li>Splitting：文本分割器把 Documents 切分为指定大小的分割，我把它们称为“文档块”或者“文档片”。</li>
<li>Storage：将上一步中分割好的“文档块”以“嵌入”（Embedding）的形式存储到向量数据库（Vector DB）中，形成一个个的“嵌入片”。</li>
<li>Retrieval：应用程序从存储中检索分割后的文档（例如通过比较余弦相似度，找到与输入问题类似的嵌入片）。</li>
<li>Synthesis：把问题和相似的嵌入片传递给语言模型（LLM），使用包含问题和检索到的分割的提示生成答案。</li>
</ol>
<p>我们使用了 <code>OpenAIEmbeddings</code> 来生成嵌入，然后使用 <code>Qdrant</code> 这个向量数据库来存储嵌入</p>
<p>词嵌入（Word Embedding）：将文本编码成向量。</p>
<blockquote>
<p>LangChain中的 <code>Embeddings</code> 类是设计用于与文本嵌入模型交互的类。这个类为所有这些提供者提供标准接口。</p>
</blockquote>
<ol>
<li><code>embed_documents</code> 方法，为文档创建嵌入。接收多个文本作为输入，意味着你可以一次性将多个文档，即被搜索的内容，转换为它们的向量表示。</li>
<li><code>embed_query</code> 方法，为查询创建嵌入。只接收一个文本作为输入，通常是用户的搜索查询。</li>
</ol>
<h3 id="存储嵌入"><a href="#存储嵌入" class="headerlink" title="存储嵌入"></a>存储嵌入</h3><p><code>CacheBackedEmbeddings</code>：将嵌入缓存在键值存储中。</p>
<p><strong>通过向量数据库（Vector Store）来保存</strong></p>
<p>非结构化查询：</p>
<p>通过向量存储检索器 <code>VectorstoreIndexCreator</code> 创建索引，在索引的 <code>query</code> 方法中，通过 <code>vectorstore</code> 类的 <code>as_retriever</code> 方法完成检索任务。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入文档加载器模块，并使用 TextLoader 来加载文本文件</span></span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line">loader = TextLoader(<span class="string">&#x27;LangChainSamples/OneFlower/易速鲜花花语大全.txt&#x27;</span>, encoding=<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 VectorstoreIndexCreator 来从加载器创建索引</span></span><br><span class="line"><span class="keyword">from</span> langchain.indexes <span class="keyword">import</span> VectorstoreIndexCreator</span><br><span class="line">index = VectorstoreIndexCreator().from_loaders([loader])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义查询字符串, 使用创建的索引执行查询</span></span><br><span class="line">query = <span class="string">&quot;玫瑰花的花语是什么？&quot;</span></span><br><span class="line">result = index.query(llm, query)</span><br><span class="line"><span class="built_in">print</span>(result) <span class="comment"># 打印查询结果</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="comment"># 进行文本分割</span></span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> Qdrant</span><br><span class="line"></span><br><span class="line"><span class="comment"># 带有 Qdrant 的向量索引创建器</span></span><br><span class="line">index_creator = VectorstoreIndexCreator(</span><br><span class="line">    vectorstore_cls=Qdrant,</span><br><span class="line">    embedding,</span><br><span class="line">    text_splitter=CharacterTextSplitter(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">0</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>一种高效地管理和定位文档信息的方法，确保每个文档具有唯一标识并便于检索。</p>
<blockquote>
<p>LangChain 利用了记录管理器（RecordManager）来跟踪哪些文档已经被写入向量存储。</p>
<p>在进行索引时，API 会对每个文档进行哈希处理，确保每个文档都有一个唯一的标识。这个哈希值不仅仅基于文档的内容，还考虑了文档的元数据。确保了即使文档经历了多次转换或处理，也能够精确地跟踪它的状态和来源，确保文档数据被正确管理和索引。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/indexing/">LangChain中 Indexing 的说明</a></p>
<h2 id="Agent-框架"><a href="#Agent-框架" class="headerlink" title="Agent 框架"></a>Agent 框架</h2><p>让ChatGPT自己生成这些引导文本</p>
<h4 id="模拟代理"><a href="#模拟代理" class="headerlink" title="模拟代理"></a>模拟代理</h4><p><strong>CAMEL 交流式代理</strong></p>
<p>交流式代理 + 角色扮演 + 启示式提示 <code>inception prompting</code></p>
<p>明确的交互规范，如一次只能给出一个指令，解决方案必须具有详细的解释。使用 “Solution: ” 开始输出解决方案，等等。有助于保持对话的清晰性和高效性。</p>
<blockquote>
<p>CAMEL中提示的设计更加复杂和细致，更像是一种交互协议或规范。这种设计在一定程度上提高了AI与AI之间自主合作的能力，并能更好地模拟人类之间的交互过程。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.17760">CAMEL：用于大规模语言模型社会的“心智”探索的交流式代理</a></p>
<h4 id="自主代理"><a href="#自主代理" class="headerlink" title="自主代理"></a>自主代理</h4><p>能够在无需外部干预的情况下自主运行，这在真实世界的应用中具有巨大的价值。</p>
<ul>
<li><code>Auto-GPT</code>: 自动链接多个任务。多步提示过程，将目标分解为子任务。</li>
<li><code>BabyAGI</code>: 自动生成和执行任务，而且还可以根据完成的任务结果生成新任务，并且可以实时确定任务的优先级。</li>
<li><code>HuggingGPT</code>: 自动生成计划，并处理多个复杂的AI任务。允许HuggingGPT持续接入。</li>
</ul>
<p>BabyAGI 完成任务过程：</p>
<blockquote>
<p>向系统提出一个目标之后，它将不断优先考虑需要实现或完成的任务，以实现该目标。具体来说，系统将形成任务列表，从任务列表中拉出优先级最高的第一个任务，使用 OpenAI API 根据上下文将任务发送到执行代理并完成任务，一旦这些任务完成，它们就会被存储在内存（或者Pinecone这类向量数据库）中，然后，根据目标和上一个任务的结果创建新任务并确定优先级。 </p>
</blockquote>
<p><img src="/07cbca8cfff33ffa231830eff145556c.webp" alt="alt text"></p>
<p>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</p>
<p>使其能够根据用户请求自动生成计划，并使用外部模型，从而整合多模态感知能力，并处理多个复杂的AI任务。此外，这种流程还允许HuggingGPT持续从任务特定的专家模型中吸收能力，从而实现可增长和可扩展的AI能力。</p>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.10439v2">开放式表格与文本问题回答</a>，Chen, W., Chang, M.-W., Schlinger, E., Wang, W., &amp; Cohen, W. W. (2021). Open Question Answering over Tables and Text. ICLR 2021.</li>
<li>Open AI的GPT-3模型：<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">大模型是少样本学习者</a>, Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … &amp; Agarwal, S. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.01652">微调后的语言模型是零样本学习者</a>, Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., &amp; Le, Q. V. (2022). Finetuned Language Models Are Zero-Shot Learners. Proceedings of the International Conference on Learning Representations (ICLR 2022).</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.12837">对示例角色的重新思考：是什么使得上下文学习有效？</a>, Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., &amp; Zettlemoyer, L. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022).</li>
<li>2022 年，在 Google 发布的论文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>中首次提出思维链（Chain of Thought），Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., &amp; Zhou, D. (2023). Self-Consistency Improves Chain of Thought Reasoning in Language Models. Proceedings of the International Conference on Learning Representations (ICLR). arXiv preprint arXiv:2203.11171.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.10601">论文，思维之树：使用大型语言模型进行深思熟虑的问题解决</a>，Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., &amp; Narasimhan, K. (2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv preprint arXiv:2305.10601.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.03629">ReAct：在语言模型中协同推理和行动</a>，Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp; Cao, Y. (2023). ReAct: Synergizing Reasoning and Acting in Language Models. arXiv preprint arXiv:2210.03629</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.09014">ART：大型语言模型的自动多步推理和工具使用</a>，Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., &amp; Ribeiro, M. T. (2023). ART: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.04091">“计划与解决”提示：通过大型语言模型改进Zero-Shot链式思考推理</a> Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., &amp; Lim, E.-P. (2023). Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. arXiv preprint arXiv:2305.04091.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.11916">Large Language Models are Zero-Shot Reasoners</a></li>
<li>论文 <a href="https://link.juejin.cn/?target=https://arxiv.org/pdf/2303.17580.pdf" title="https://arxiv.org/pdf/2303.17580.pdf">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a></li>
</ul>

                <hr>
                

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2024/11/17/数据库系统/" data-toggle="tooltip" data-placement="top"
                           title="数据库系统">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2024/11/08/理解与使用CSS/" data-toggle="tooltip" data-placement="top"
                           title="理解与使用CSS">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                

                


                <!--加入新的评论系统-->
                

                

            </div>

            <div class="hidden-xs col-sm-3 toc-col">
                <div class="toc-wrap">
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#LangChain%E5%8A%9F%E8%83%BD%E6%80%BB%E7%BB%93"><span class="toc-text">LangChain功能总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-I-O-%E8%BF%87%E7%A8%8B"><span class="toc-text">Model I&#x2F;O 过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B"><span class="toc-text">提示工程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-Chain-of-Thought"><span class="toc-text">使用 Chain of Thought</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E7%94%A8-OpenAI-API"><span class="toc-text">调用 OpenAI API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E8%A7%A3%E6%9E%90"><span class="toc-text">输出解析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Chain"><span class="toc-text">Chain</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ConversationChain"><span class="toc-text">ConversationChain</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%90%86"><span class="toc-text">代理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ReAct%E6%A1%86%E6%9E%B6"><span class="toc-text">ReAct框架</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-arXiv-%E5%B7%A5%E5%85%B7"><span class="toc-text">使用 arXiv 工具</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAG-%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84"><span class="toc-text">RAG 应用架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%B5%8C%E5%85%A5"><span class="toc-text">存储嵌入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95"><span class="toc-text">索引</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Agent-%E6%A1%86%E6%9E%B6"><span class="toc-text">Agent 框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F%E4%BB%A3%E7%90%86"><span class="toc-text">模拟代理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E4%B8%BB%E4%BB%A3%E7%90%86"><span class="toc-text">自主代理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BA%E6%96%87"><span class="toc-text">论文</span></a></li></ol>
                </div>
            </div>
        </div>

        <div class="row">
            <!-- Sidebar Container -->

            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                

                <!-- Friends Blog -->
                
            </div>
        </div>

    </div>
</article>







<!-- Footer -->
<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 text-center">
                <br>
                <ul class="list-inline text-center">
                
                
                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Alexiang 2024
                    <br>
                    <span id="busuanzi_container_site_pv" style="font-size: 12px;">PV: <span id="busuanzi_value_site_pv"></span> Times</span>
                    <br>
                    Theme by <a target="_blank" rel="noopener" href="https://haojen.github.io/">Haojen Ma</a>
                </p>

            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/blog.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://example.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>



<!-- Google Analytics -->



<!-- Baidu Tongji -->


<!-- swiftype -->
<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','','2.0.0');
</script>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<!--wechat title img-->
<img class="wechat-title-img" src="">
</body>

</html>
